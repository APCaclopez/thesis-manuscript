[#conclusion]
== Conclusion and Recommendations

This chapter details the final outcomes of the study. This includes addressing the research questions posed and objectives outlined. It also details possible directions that future studies may take.

=== Conclusion

Sign languages are not well-known in the Philippines, and this study aims to address the issue by developing and evaluating a model that can be used in learning applications. This model aims to assess the execution of a sign and provide feedback regarding mistakes and takes into account facial features, which have been proven to be important for recognizing Filipino Sign Language (FSL).

The developed model proved to be decent, achieving an accuracy of 65.67%, precision of 62.47%, and recall of 79.53%. To assess the effects of the inclusion of the facial features module, the facial features module was removed and assess as well. This model achieved an accuracy of 65.05%, precision of 60.99%, and recall of 84.39% showing that the inclusion of the facial features module improves accuracy and precision at the cost of lower recall. The ability of the model to determine the difference between two videos of gestures was assessed, albeit with a less than ideal dataset. This showed that location feedback proved to be the most reliable with a precision of 69.00% and recall of 83.00%, while the other modules require further development.

These results show that while the model shows potential, further developments and improvements are required to make the model suitable for learning purposes. Models made for providing learner feedback aim to provide a learning experience that translation models do not consider. As such, the goal is for these feedback models is to rival the performance of translation models. This study does not achieve this goal, as research into models that provide translations is more developed than research into models that provide feedback. However, it does explore the idea and provide possible future directions to explore.

=== Recommendations

While these results show that the model is somewhat capable, there is much room to improve. Possible future directions include the following:

* *Further fine tune and optimize the thresholds.* This study explores the use of a new model for determining the similarity between two videos of sign language gestures. This model is composed of four modules, where each calculates a similarity measure. Thresholds must be set to define what constitutes a significant difference between two gestures. Due to the nature of chaining these different modules together, it is unclear how to balance the metrics as one experiments with different thresholds. Further exploration on the optimal thresholds may yield better results.
* *Use more sophisticated shape-comparing methods for the hand shape and facial expression modules.* The hand shape and facial expression modules exhibited lower metrics than the location module and relied on simpler techniques. There exist more sophisticated techniques specifically designed to compare shapes, like object keypoint similarity (OKS). It may be beneficial to explore the use of these techniques in the modules related to shape comparison.
* *Use more sophisticated dynamic time warping variations for the movement module.* The movement module was found to be less impactful than the other modules. This may be due to the diminishing effectiveness of the dynamic time warping algorithm for longer series. Though the samples were segmented to combat this, the segmentation algorithm used did not consider the alignment of features (i.e., peaks and valleys) in the series. Future work may attempt to use improved variations on the dynamic time warping algorithm that involve more intelligent segmentation methods. As hand shape and facial expression may change throughout the course of the gesture, it may also be beneficial to allow the movement module to inform the frame sampling method used in the hand shape and facial expression modules to ensure that the frames that are compared are taken from the same point in time.
* *Generate more extensive and detailed datasets.* A common problem found in less explored research areas is the lack of datasets, and Filipino Sign Language (FSL) is no exception. While the dataset used in this study contains 105 gestures, there are only around 20 videos per gesture. This prevents the use of techniques that involve the training of more sophisticated machine learning models. There also exist no datasets that label the differences between different gestures, which can be used to assess the feedback of the model.
* *Assess the feedback of the model.* The goal of this model is to provide feedback to learners. However, due to limiting factors, only the model's ability to identify whether two videos are of the same gesture was tested. This does not assess the specific feedback given by the model. In other words, only a binary classification task was used to assess the model, while a multiclass classification task would be more suitable. However, this would require a more extensive and detailed dataset where differences between two videos are identified.
* *Apply the model in an application.* Finally, this study only explores the creation of a feedback model. The assessments conducted involve the use of datasets, not real-world use. The implementation and integration of the model within a usable application may be explored in future works.
