[#methodology]
== Methodology

This chapter details the specific processes involved in achieving the objectives outlined in <<objectives>>. This study is modeled after the work of Paudyal et al. cite:[paudyal19]. As such, much of this study follows similar methods. However, it also introduces modifications aimed to improve the model, specifically in the context of Filipino Sign Language (FSL). Other modifications are also introduced to take advantage of better technologies that have been made available since the publishing of the aforementioned previous work.

[#data]
=== Data Collection

This study will be using a public dataset known as FSL-105 cite:[tupal23]. It contains 105 different Filipino Sign Language gestures and was created by a team from De La Salle University. In total, it consists of 2130 4-second clips. 16 FSL gestures will be selected as the sample for testing the model's ability to determine correct and incorrect attempts, as these signs are more dynamic compared to the rest of the dataset. To assess the feedback, a smaller subsample of five FSL gestures will be selected, and the differences between their components will be manually identified.

[#preprocessing]
=== Data Preprocessing

// see src/manuscript.adoc for explanation how to make figures
:fig-label-pose: {fig-label} {counter:fig}
.{fig-label-pose}. Example pose estimation with MediaPipe cite:[mediapipePose]
[#fig-pose]
image::../images/mediapipe_pose.png[MediaPipe pose estimation]

The data described in <<data>> will not be directly used by the model. Instead of processing videos, the model will process pose data estimated from videos, as seen in <<fig-pose,{fig-label-pose}>>. This pose data will be in the form of keypoints, or the locations of the signer's joints within the video. Unlike the work of Paudyal et al. cite:[paudyal19], which relies on PoseNet, this will be done with the use Google's MediaPipe framework. However, this data may still contain noise in the form of varying scale and location introduced by variations in camera location and settings. To account for this, the data will be normalized; joints will be scaled and translated to be a consistent size within the center of the frame.

[#model]
=== Model Development

// see src/manuscript.adoc for explanation how to make figures
:fig-label-framework: {fig-label} {counter:fig}
.{fig-label-framework}. Techniques and Technologies
[#fig-framework]
image::../images/framework.png[Framework]

<<fig-framework,{fig-label-framework}>> shows the framework of the model to be created, as well as its differences from the model developed by Paudyal et al. cite:[paudyal19]. Paudyal et al. showcased the effectiveness of a model composed of three modules, each of which analyzes a specific component of gestures: hand location, movement, and shape. This study introduces a fourth module for facial features, as these are an important component in Filipino Sign Language cite:[montefalcon23]. These modules produce a vector that describes the gesture performed. To determine the correctness of a learner's gesture, it will be compared to a reference gesture using cosine similarity as a measure of similarity. Failure to meet a certain threshold of similarity set for each module will provide feedback to the learner.

Each module is developed independently. However, in the context of an application, these modules are to be invoked sequentially: a module is only invoked in the case that the preceding module determines that the gesture is correct. This design is intentional due to the following factors. First, running only one module at a time when needed reduces computational load; this allows for faster feedback. Second, it allows learners to focus on incremental improvements. The gesture is deemed correct if all modules determine that the gesture is correct.

Other than allowing for detailed feedback, the use of feature extraction methods, as opposed to more sophisticated machine learning models, allows for the trivial addition of gestures and avoids requiring extensive datasets. Rather than having to retrain models when new gestures are added, new reference gestures simply need to be added to compare the learner's gestures to.

[#location]
==== Hand Location

// see src/manuscript.adoc for explanation how to make figures
:fig-label-bucket: {fig-label} {counter:fig}
.{fig-label-bucket}. "`Location bucketing`" example cite:[paudyal19]
[#fig-bucket]
image::../images/bucket.png[location bucketing]

To determine the signer's hand location, the frame will be divided into six sections (dubbed "`buckets`") based on the location of the signer's shoulders. This is because it is the relative positions of the hands that matter--not the absolute positions cite:[bajernee20]. The presence of hands will be tracked within each bucket throughout the gesture, as seen in <<fig-bucket,{fig-label-bucket}>>. Each bucket will be a component of a vector, resulting in a vector with six components. This can be thought of as an arrow "`pointing`" somewhere in a six-dimensional space. The resulting vector from the learner's gesture will then be compared to the reference gesture using cosine similarity, as one of the applications of the cosine function, at a high level, is to determine how similarly two multidimensional vectors are "`pointing`".

[#movement]
==== Hand Movement

// see src/manuscript.adoc for explanation how to make figures
:fig-label-dtw: {fig-label} {counter:fig}
.{fig-label-dtw}. Illustration of Euclidean distance vs dynamic time warping cite:[tavenard21]
[#fig-dtw]
image::../images/dtw_vs_euc.png[Euclidean distance vs dynamic time warping]

To determine the correctness of the hand movement, dynamic time warping will be performed between the learner's gesture and the reference gesture. This technique maps two sequences to each other based on similar features, as seen in <<fig-dtw,{fig-label-dtw}>>. The distance calculated between the two will determine the similarity of the two motions. However, before this is performed, this module requires an additional preprocessing step. This module is affected by variations in video lengths; to address this, longer videos will be down-sampled to keep lengths consistent.

[#shape]
==== Hand Shape

// see src/manuscript.adoc for explanation how to make figures
:fig-label-hand: {fig-label} {counter:fig}
.{fig-label-hand}. Example of hand keypoint estimation with MediaPipe cite:[mediapipeHand]
[#fig-hand]
image::../images/mediapipe_hand.png[MediaPipe gesture estimation]

Using the pose estimation obtained in <<preprocessing>>, the videos will be cropped to focus on the signer's hands. At this point, Paudyal et al. cite:[paudyal19] use a convolutional neural network for feature extraction. However, this study will take advantage of MediaPipe's hand landmark estimation model to estimate the hand shape as well, as seen in <<fig-hand,{fig-label-hand}>>. Similar preprocessing steps to normalize the scale and position will be performed. The similarity between the learner's hand shape and the reference hand shape will be determined using cosine similarity. Cosine similarity is described in <<location>>, and can be applied here as well--though with more components/dimensions--as the hand shape may be stored as a vector composed of its keypoints' coordinates.

[#face]
==== Facial Features

Montefalcon et al. cite:[montefalcon23] showed that facial features affect the recognition of Filipino Sign Language (FSL). As such, this study introduces the facial feature module to aid in providing feedback to FSL learners. This module works similarly to the hand shape module. The pose estimation obtained in <<preprocessing>> includes facial features. This data will be normalized, and specific keypoints will be selected based on the work of Huang and Huang cite:[huang97]. The similarity between the learner's gesture and the reference gesture will then be calculated similarly to the <<shape>>.

[#assessment]
=== Model Assessment

To assess the effectiveness of the model, the comparison of a reference video and a video of learner's attempt will be simulated by pairing different videos from the dataset. Pairs of different videos of the same gesture will serve as correct attempts, while pairs of videos of different gestures will serve as incorrect attempts. Due to the nature of this pairing, there will be far fewer correct samples than incorrect samples, and incorrect samples will be undersampled to avoid a class imbalance. The model will then determine whether the signs were correctly paired. The accuracy, precision, recall, and F1 scores will be calculated and analyzed.

While these measures assess the ability of the model to determine if two videos are of the same gesture, they do not assess if the incorrect component is correctly identified. To assess the feedback a similar pairing process will be done, and feedback outputted by the model will be compared to another subset of the dataset. A smaller subset of five gestures will be selected, and labels will be made identifying the feedback required. Similar metrics will be used to measure the effectivenesss of the model in this aspect.
