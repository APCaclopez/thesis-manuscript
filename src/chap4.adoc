[#rnd]
== Results and Discussion

This chapter details the data gathered from following the procedures outlined in <<methodology>>. It also discusses insights regarding this data.

[#results]
=== Results

A model was successfully created as described in <<methodology>>: <<model>> and assessed as described in <<methodology>>: <<assessment>>. Testing was conducted on a subset of 16 gestures from the FSL-105 dataset cite:[tupal23], in which incorrect pairings were undersampled to prevent a class imbalance. Testing was conducted on a total of 480 samples (latexmath:[16\text{ gestures} \cdot \left(15\text{ correct} + 15\text{ incorrect}\right)]).

:table-label-modules: {table-label} {counter:table}
.{table-label-modules}. Individual module metrics.
[#table-module-results]
[%header,cols=6*]
|===
s|Module
s|Threshold
s|Accuracy
s|Precision
s|Recall
s|F1 Score

h|Location
>|0.975
>|69.96%
>|66.43%
>|56.89%
>|72.24%

h|Movement
>|0.500
>|60.63%
>|56.95%
>|71.82%
>|68.86%

h|Shape
>|0.920
>|58.75%
>|55.80%
>|71.63%
>|67.11%

h|Face
>|0.995
>|57.50%
>|55.29%
>|68.12%
>|64.83%
|===

<<table-module-results,{table-label-modules}>> shows the metrics obtained by each individual module when used on the testing data. As the modules simply output a measure of similarity, thresholds must be set to classify correct and incorrect attempts. The thresholds used were experimentally determined prior to testing.

:table-label-models: {table-label} {counter:table}
.{table-label-models}. Model metrics with and without face module.
[#table-model-results]
[%header,cols=5*]
|===
s|Model
s|Accuracy
s|Precision
s|Recall
s|F1 Score

h|With face
>|67.50%
>|81.82%
>|33.33%
>|58.06%

h|Without face
>|70.42%
>|75.26%
>|43.20%
>|67.28%
|===

<<table-model-results,{table-label-models}>> shows the metrics achieved by combining the modules. Compared to the individual modules seen in <<table-module-results,{table-label-modules}>>, combining the modules led to more precise predictions at the cost of worse recall. To determine the impact of including a facial features module, a model which excludes this module was also tested. As seen, the inclusion of a face module gained precision at the cost of all other metrics.

To assess the feedback of the model, various samples of five gestures were paired in a similar fashion, resulting in 375 samples (15 samples for each possible latexmath:[5 \cdot 4] combinations of gestures). However, this dataset is imbalanced. Of the 20 possible gesture pairings, 14 pairings represented expected "hand location" feedback. Only four pairings represented expected "hand/arm movement" feedback; two pairings represented expected "hand shape" feedback. No samples represented expected "facial expression" feedback.

:table-label-feedback: {table-label} {counter:table}
.{table-label-feedback}. Model feedback metrics with face module.
[#table-feedback]
[%header,cols=4*]
|===
s|Component
s|Precision
s|Recall
s|F1 Score

h|None
>|33.33%
>|40.00%
>|36.36%

h|Location
>|60.00%
>|64.29%
>|62.07%

h|Movement
>|50.00%
>|50.00%
>|50.00%

h|Shape
>|
>|
>|

h|Face
>|
>|0%
>|
|===

<<table-feedback,{table-label-feedback}>> shows the precision, recall, and f1 scores of the model for each of its expected outputs. Due to the imbalance in the dataset, some classes were uncomputable or received metrics of 0%. Due to the lack of samples which warrant facial expression feedback, the inclusion of a facial feature module cannot be assessed.

[#discussion]
=== Discussion

The thresholds set for the modules were experimentally chosen in an attempt to balance the different metrics. The model works by chaining together modules by only deeming a pairing correct if all modules output that the pairing is correct. Each additional module only identifies more negative samples. Because of this, while it is ideal for each module to achieve high accuracy and precision, it is also important to maintain a high recall to limit the number of false negatives.

As seen in <<table-module-results,{table-label-modules}>>, the individual modules prove to be somewhat capable of classifying whether pairs of videos are of the same gesture or not, achieving decent metrics. Meanwhile, as seen in <<table-model-results,{table-label-models}>>, Combining the modules together achieves greater precision than any individual module, while maintaining decent accuracy, at the cost of decreased recall and F1 score. This shows that combining the modules better limits false positives at the cost of more false negatives.

To determine the effects of the inclusion of the facial features module, a model without the face module was also tested. Compared to individual modules, this model also achieves better precision, while maintaining accuracy and F1 score, at the cost of worse recall. Compared to this, the model which includes the facial features module achieves better precision at the cost of all other metrics. This shows that the addition of the facial features module further limits false positives at the cost of more false negatives.

While these results prove the model is somewhat effective, they fall short of the results obtained by Paudyal et al. cite:[paudyal19]. This may be due to the use of a different dataset, as well as discrepancies in the processing of the data.

Meanwhile, it is difficult to assess the model's abiliy to classify the warranted feedback due to the highly imbalanced dataset. A majority of the dataset represents samples that warrant location feedback, which leaves the other classes somewhat unreliable and even uncomputable. With this imbalance in mind, as seen in <<table-feedback,{table-label-feedback}>>, the model seems somewhat reliable when it comes to location feedback, with precision, recall, and and F1 score of above 60%. This may be due to the location module exhibiting the best metrics among all the modules. Among the feedback involving other underrepresented components, feedback involving movement was not far behind, with all metrics at 50%.
